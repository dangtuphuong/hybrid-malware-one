import joblib
import json
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
import xgboost as xgb
from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import LabelEncoder
import uuid


def train_models(file, separator, feature_columns, target_column):
    try:
        # Load dataset
        data = pd.read_csv(file, sep=separator or ',')
    except FileNotFoundError:
        raise FileNotFoundError("Dataset file not found.")
    except pd.errors.EmptyDataError:
        raise ValueError("Dataset file is empty.")
    except Exception as e:
        raise ValueError(
            f"An error occurred while loading the dataset: {str(e)}")

    # Drop rows with any missing values
    data = data.dropna(subset=feature_columns + [target_column])

    if data.empty:
        raise ValueError(
            "No valid data available after dropping missing values.")

    # Split the data into features and target
    X = data[feature_columns]
    y = data[target_column]

    # Check if the target variable contains categorical string labels
    if y.dtype == 'object':
        le = LabelEncoder()
        y = le.fit_transform(y)

    # Split the data into training and test sets
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42)

    ### Random Forest Model ###
    try:
        rf_model = RandomForestClassifier(n_estimators=200, random_state=42)
        rf_model.fit(X_train, y_train)

        rf_test_preds = rf_model.predict(X_test)
        rf_metrics = {
            'accuracy': accuracy_score(y_test, rf_test_preds),
            'precision': precision_score(y_test, rf_test_preds, average='macro', zero_division=1),
            'recall': recall_score(y_test, rf_test_preds, average='macro', zero_division=1),
            'f1': f1_score(y_test, rf_test_preds, average='macro', zero_division=1),
            'classification_report': json.dumps(
                classification_report(
                    y_test, rf_test_preds, zero_division=1, output_dict=True),
                separators=(',', ':')
            )
        }

        print("\n### Random Forest Validation Metrics ###")
        print(f"Accuracy: {rf_metrics['accuracy']}")
        print(f"Precision: {rf_metrics['precision']}")
        print(f"Recall: {rf_metrics['recall']}")
        print(f"F1 Score: {rf_metrics['f1']}")
        print(rf_metrics['classification_report'])

    except Exception as e:
        raise ValueError(f"Error training Random Forest model: {str(e)}")

    ### XGBoost Model ###
    try:
        xgb_model = xgb.XGBClassifier(
            n_estimators=200, eval_metric='logloss', random_state=42)
        xgb_model.fit(X_train, y_train)

        xgb_test_preds = xgb_model.predict(X_test)
        xgb_metrics = {
            'accuracy': accuracy_score(y_test, xgb_test_preds),
            'precision': precision_score(y_test, xgb_test_preds, average='macro', zero_division=1),
            'recall': recall_score(y_test, xgb_test_preds, average='macro', zero_division=1),
            'f1': f1_score(y_test, xgb_test_preds, average='macro', zero_division=1),
            'classification_report': json.dumps(
                classification_report(
                    y_test, xgb_test_preds, zero_division=1, output_dict=True),
                separators=(',', ':')
            )
        }

        print("\n### XGBoost Validation Metrics ###")
        print(f"Accuracy: {xgb_metrics['accuracy']}")
        print(f"Precision: {xgb_metrics['precision']}")
        print(f"Recall: {xgb_metrics['recall']}")
        print(f"F1 Score: {xgb_metrics['f1']}")
        print(xgb_metrics['classification_report'])

    except Exception as e:
        raise ValueError(f"Error training XGBoost model: {str(e)}")

    ### Voting Ensemble Model ###
    try:
        voting_model = VotingClassifier(estimators=[
            ('rf', rf_model),
            ('xgb', xgb_model)
        ], voting='hard')

        voting_model.fit(X_train, y_train)

        voting_test_preds = voting_model.predict(X_test)
        hybrid_metrics = {
            'accuracy': accuracy_score(y_test, voting_test_preds),
            'precision': precision_score(y_test, voting_test_preds, average='macro', zero_division=1),
            'recall': recall_score(y_test, voting_test_preds, average='macro', zero_division=1),
            'f1': f1_score(y_test, voting_test_preds, average='macro', zero_division=1),
            'classification_report': json.dumps(
                classification_report(
                    y_test, voting_test_preds, zero_division=1, output_dict=True),
                separators=(',', ':')
            )
        }

        print("\n### Voting Ensemble Model (RF + XGBoost) ###")
        print(f"Accuracy: {hybrid_metrics['accuracy']}")
        print(f"Precision: {hybrid_metrics['precision']}")
        print(f"Recall: {hybrid_metrics['recall']}")
        print(f"F1 Score: {hybrid_metrics['f1']}")
        print(hybrid_metrics['classification_report'])

    except Exception as e:
        raise ValueError(f"Error training Voting Ensemble model: {str(e)}")

    # Load existing models, if the file exists
    try:
        existing_models = joblib.load('models_with_ids.pkl')
    except FileNotFoundError:
        existing_models = {}

    # Define new models with unique IDs
    model_ids = {
        'rf': str(uuid.uuid4()),
        'xgb': str(uuid.uuid4()),
        'hybrid': str(uuid.uuid4())
    }

    existing_models[model_ids['rf']] = {
        'model': rf_model, 'label': 'Random Forest'}
    existing_models[model_ids['xgb']] = {
        'model': xgb_model, 'label': 'XGBoost'}
    existing_models[model_ids['hybrid']] = {
        'model': voting_model, 'label': 'Hybrid'}

    # Save the models with unique IDs back to the file
    joblib.dump(existing_models, 'models_with_ids.pkl')

    return rf_metrics, xgb_metrics, hybrid_metrics, model_ids
