import joblib
import json
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
import xgboost as xgb
from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score, multilabel_confusion_matrix
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import LabelEncoder
import uuid


def train_models(file, separator, feature_columns, target_column, train_models_str='rf,xgb,hybrid', cv_folds=3):
    sep = separator if separator else ','
    option_models = ""

    if isinstance(train_models_str, str):
        option_models = train_models_str.split(',')

    has_rf = 'rf' in option_models
    has_xgb = 'xgb' in option_models
    has_hybrid = 'hybrid' in option_models

    try:
        # Load dataset
        data = pd.read_csv(file, sep=sep)
    except FileNotFoundError:
        raise FileNotFoundError("Dataset file not found.")
    except pd.errors.EmptyDataError:
        raise ValueError("Dataset file is empty.")
    except Exception as e:
        raise ValueError(
            f"An error occurred while loading the dataset: {str(e)}")

    # Handle missing values
    data = data.ffill()

    if data.empty:
        raise ValueError(
            "No valid data available after dropping missing values.")

    # Split the data into features and target
    X = data[feature_columns].values
    y = data[target_column].values

    # Calculate and display malware type distribution
    malware_distribution = data[target_column].value_counts()
    print(f"\n### Malware Type Distribution ###\n{malware_distribution}")

    # Check if the target variable contains categorical string labels
    label_encoder = None
    if y.dtype == 'object':
        label_encoder = LabelEncoder()
        y = label_encoder.fit_transform(y)

    # Split the data into training and test sets
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=0)

    if label_encoder:
        y_test_decoded = label_encoder.inverse_transform(y_test)
    else:
        y_test_decoded = y_test

    rf_metrics = None
    xgb_metrics = None
    hybrid_metrics = None

    ### Random Forest Model ###
    if has_rf or has_hybrid:
        try:
            rf_model = RandomForestClassifier(
                n_estimators=200, max_depth=20, min_samples_leaf=1, min_samples_split=2, random_state=0)
            rf_model.fit(X_train, y_train)

            rf_cv_scores = cross_val_score(
                rf_model, X_train, y_train, cv=cv_folds)
            rf_test_preds = rf_model.predict(X_test)

            # Inverse-transform the encoded labels if necessary
            rf_metrics = calculate_metrics(
                y_test_decoded, rf_test_preds, label_encoder)

            print("\n### Random Forest Validation Metrics ###")
            print(f"Cross-Validation Accuracy: {np.mean(rf_cv_scores)}")
            print(f"Test Accuracy: {rf_metrics['accuracy']}")
            print(f"Precision: {rf_metrics['precision']}")
            print(f"Recall: {rf_metrics['recall']}")
            print(f"F1 Score: {rf_metrics['f1']}")
            print(rf_metrics['classification_report'])

        except Exception as e:
            raise ValueError(f"Error training Random Forest model: {str(e)}")

    ### XGBoost Model ###
    if has_xgb or has_hybrid:
        try:
            xgb_model = xgb.XGBClassifier(n_estimators=100, colsample_bytree=1.0, learning_rate=0.2,
                                          max_depth=9, subsample=0.8, eval_metric='logloss', random_state=0)
            xgb_model.fit(X_train, y_train)

            xgb_cv_scores = cross_val_score(
                xgb_model, X_train, y_train, cv=cv_folds)
            xgb_test_preds = xgb_model.predict(X_test)

            xgb_metrics = calculate_metrics(
                y_test_decoded, xgb_test_preds, label_encoder)

            print("\n### XGBoost Validation Metrics ###")
            print(f"Cross-Validation Accuracy: {np.mean(xgb_cv_scores)}")
            print(f"Test Accuracy: {xgb_metrics['accuracy']}")
            print(f"Precision: {xgb_metrics['precision']}")
            print(f"Recall: {xgb_metrics['recall']}")
            print(f"F1 Score: {xgb_metrics['f1']}")
            print(xgb_metrics['classification_report'])

        except Exception as e:
            raise ValueError(f"Error training XGBoost model: {str(e)}")

    ### Hybrid Model ###
    if has_hybrid:
        try:
            if rf_metrics and xgb_metrics:
                voting_model = VotingClassifier(
                    estimators=[('rf', rf_model), ('xgb', xgb_model)], voting='hard')
                voting_model.fit(X_train, y_train)

                hybrid_cv_scores = cross_val_score(
                    voting_model, X_train, y_train, cv=cv_folds)
                voting_test_preds = voting_model.predict(X_test)

                hybrid_metrics = calculate_metrics(
                    y_test_decoded, voting_test_preds, label_encoder)

                print("\n### Hybrid Model (RF + XGBoost) ###")
                print(
                    f"Cross-Validation Accuracy: {np.mean(hybrid_cv_scores)}")
                print(f"Test Accuracy: {hybrid_metrics['accuracy']}")
                print(f"Precision: {hybrid_metrics['precision']}")
                print(f"Recall: {hybrid_metrics['recall']}")
                print(f"F1 Score: {hybrid_metrics['f1']}")
                print(hybrid_metrics['classification_report'])

        except Exception as e:
            raise ValueError(f"Error training Hybrid model: {str(e)}")

    # Load existing models, if the file exists
    try:
        existing_models = joblib.load('models_with_ids.pkl')
    except FileNotFoundError:
        existing_models = {}

    # Define new models with unique IDs
    model_ids = {}
    if rf_metrics and has_rf:
        model_ids['rf'] = str(uuid.uuid4())
        existing_models[model_ids['rf']] = {
            'model': rf_model,
            'label': 'Random Forest',
            'label_encoder': label_encoder,
            'feature_columns': feature_columns,
            "target_column": target_column,
            'separator': sep
        }

    if xgb_metrics and has_xgb:
        model_ids['xgb'] = str(uuid.uuid4())
        existing_models[model_ids['xgb']] = {
            'model': xgb_model,
            'label': 'XGBoost',
            'label_encoder': label_encoder,
            'feature_columns': feature_columns,
            "target_column": target_column,
            'separator': sep
        }

    if hybrid_metrics and has_hybrid:
        model_ids['hybrid'] = str(uuid.uuid4())
        existing_models[model_ids['hybrid']] = {
            'model': voting_model,
            'label': 'Hybrid',
            'label_encoder': label_encoder,
            'feature_columns': feature_columns,
            "target_column": target_column,
            'separator': sep
        }

    # Save the models with unique IDs back to the file
    joblib.dump(existing_models, 'models_with_ids.pkl')

    result = [
        malware_distribution,
        rf_metrics if has_rf else None,
        xgb_metrics if has_xgb else None,
        hybrid_metrics if has_hybrid else None,
        model_ids
    ]

    return tuple(result)


def calculate_metrics(y_true, y_pred, label_encoder):
    if label_encoder:
        y_pred_decoded = label_encoder.inverse_transform(y_pred)
    else:
        y_pred_decoded = y_pred

    # Generate accuracies report
    report = get_classification_report(y_true, y_pred_decoded)

    metrics = {
        'accuracy': accuracy_score(y_true, y_pred_decoded),
        'precision': precision_score(y_true, y_pred_decoded, average='macro', zero_division=1),
        'recall': recall_score(y_true, y_pred_decoded, average='macro', zero_division=1),
        'f1': f1_score(y_true, y_pred_decoded, average='macro', zero_division=1),
        'classification_report': json.dumps(report, separators=(',', ':'))
    }
    return metrics


def get_classification_report(y_true, y_pred_decoded):
    # Generate classification report
    report = classification_report(
        y_true, y_pred_decoded, zero_division=1, output_dict=True
    )

    # Extract class labels from the report
    labels = [
        class_name
        for class_name in report.keys()
        if class_name not in ["accuracy", "macro avg", "weighted avg"]
    ]

    # Generate multilabel confusion matrix with the extracted labels
    mcm = multilabel_confusion_matrix(y_true, y_pred_decoded, labels=labels)

    # Mapped result
    mapped_result = {}

    # Loop through each class in the classification report
    for i, class_name in enumerate(labels):
        metrics_dict = report[class_name]
        tn, fp, fn, tp = mcm[i].ravel()
        total_instances = tp + fn  # Total actual instances of the class
        class_accuracy = tp / total_instances if total_instances > 0 else 0

        # Add accuracy to the metrics_dict for each class
        metrics_dict["accuracy"] = class_accuracy
        mapped_result[class_name] = metrics_dict

    # Add overall accuracy and averages
    mapped_result["overall"] = {
        "accuracy": report["accuracy"],
        "macro_avg": report["macro avg"],
        "weighted_avg": report["weighted avg"],
    }

    return mapped_result

